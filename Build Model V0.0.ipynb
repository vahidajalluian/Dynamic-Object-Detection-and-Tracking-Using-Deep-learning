{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model V0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net Data\n",
    "    \n",
    "| batch=1 | subdivisions=1 | Training |\n",
    "| :---: | :---: | :---: |\n",
    "| decay=0.0005 | angle=0 | saturation = 1.5 |\n",
    "| batch=64 | subdivisions=16 | width=608 |\n",
    "| height=608 | channels=3 | momentum=0.9 |\n",
    "|exposure = 1.5 | hue=.1 | learning_rate=0.001|\n",
    "|burn_in=1000| max_batches = 500200| policy=steps|  \n",
    "|steps=400000,450000|scales=.1,.1||\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, LSTM, Dense, Conv2D, LeakyReLU, ZeroPadding2D\n",
    "from keras.layers import concatenate\n",
    "from keras.activations import linear\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adagrad\n",
    "from keras.utils import plot_model\n",
    "from keras.engine.topology import Layer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_class = 80\n",
    "max_box_per_image = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YoloLayer(Layer):\n",
    "    def __init__(self, anchors, max_grid, batch_size, warmup_batches, ignore_thresh, \n",
    "                    grid_scale, obj_scale, noobj_scale, xywh_scale, class_scale, \n",
    "                    **kwargs):\n",
    "        # make the model settings persistent\n",
    "        self.ignore_thresh  = ignore_thresh\n",
    "        self.warmup_batches = warmup_batches\n",
    "        self.anchors        = tf.constant(anchors, dtype='float', shape=[1,1,1,3,2])\n",
    "        self.grid_scale     = grid_scale\n",
    "        self.obj_scale      = obj_scale\n",
    "        self.noobj_scale    = noobj_scale\n",
    "        self.xywh_scale     = xywh_scale\n",
    "        self.class_scale    = class_scale        \n",
    "\n",
    "        # make a persistent mesh grid\n",
    "        max_grid_h, max_grid_w = max_grid\n",
    "\n",
    "        cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(max_grid_w), [max_grid_h]), (1, max_grid_h, max_grid_w, 1, 1)))\n",
    "        cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
    "        self.cell_grid = tf.tile(tf.concat([cell_x,cell_y],-1), [batch_size, 1, 1, 3, 1])\n",
    "\n",
    "        super(YoloLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(YoloLayer, self).build(input_shape)  # Be sure to call this somewhere!\n",
    "\n",
    "    def call(self, x):\n",
    "        input_image, y_pred, y_true, true_boxes = x\n",
    "\n",
    "        # adjust the shape of the y_predict [batch, grid_h, grid_w, 3, 4+1+nb_class]\n",
    "        y_pred = tf.reshape(y_pred, tf.concat([tf.shape(y_pred)[:3], tf.constant([3, -1])], axis=0))\n",
    "        \n",
    "        # initialize the masks\n",
    "        object_mask     = tf.expand_dims(y_true[..., 4], 4)\n",
    "\n",
    "        # the variable to keep track of number of batches processed\n",
    "        batch_seen = tf.Variable(0.)        \n",
    "\n",
    "        # compute grid factor and net factor\n",
    "        grid_h      = tf.shape(y_true)[1]\n",
    "        grid_w      = tf.shape(y_true)[2]\n",
    "        grid_factor = tf.reshape(tf.cast([grid_w, grid_h], tf.float32), [1,1,1,1,2])\n",
    "\n",
    "        net_h       = tf.shape(input_image)[1]\n",
    "        net_w       = tf.shape(input_image)[2]            \n",
    "        net_factor  = tf.reshape(tf.cast([net_w, net_h], tf.float32), [1,1,1,1,2])\n",
    "        \n",
    "        \"\"\"\n",
    "        Adjust prediction\n",
    "        \"\"\"\n",
    "        pred_box_xy    = (self.cell_grid[:,:grid_h,:grid_w,:,:] + tf.sigmoid(y_pred[..., :2]))  # sigma(t_xy) + c_xy\n",
    "        pred_box_wh    = y_pred[..., 2:4]                                                       # t_wh\n",
    "        pred_box_conf  = tf.expand_dims(tf.sigmoid(y_pred[..., 4]), 4)                          # adjust confidence\n",
    "        pred_box_class = y_pred[..., 5:]                                                        # adjust class probabilities      \n",
    "\n",
    "        \"\"\"\n",
    "        Adjust ground truth\n",
    "        \"\"\"\n",
    "        true_box_xy    = y_true[..., 0:2] # (sigma(t_xy) + c_xy)\n",
    "        true_box_wh    = y_true[..., 2:4] # t_wh\n",
    "        true_box_conf  = tf.expand_dims(y_true[..., 4], 4)\n",
    "        true_box_class = tf.argmax(y_true[..., 5:], -1)         \n",
    "\n",
    "        \"\"\"\n",
    "        Compare each predicted box to all true boxes\n",
    "        \"\"\"        \n",
    "        # initially, drag all objectness of all boxes to 0\n",
    "        conf_delta  = pred_box_conf - 0 \n",
    "\n",
    "        # then, ignore the boxes which have good overlap with some true box\n",
    "        true_xy = true_boxes[..., 0:2] / grid_factor\n",
    "        true_wh = true_boxes[..., 2:4] / net_factor\n",
    "        \n",
    "        true_wh_half = true_wh / 2.\n",
    "        true_mins    = true_xy - true_wh_half\n",
    "        true_maxes   = true_xy + true_wh_half\n",
    "        \n",
    "        pred_xy = tf.expand_dims(pred_box_xy / grid_factor, 4)\n",
    "        pred_wh = tf.expand_dims(tf.exp(pred_box_wh) * self.anchors / net_factor, 4)\n",
    "        \n",
    "        pred_wh_half = pred_wh / 2.\n",
    "        pred_mins    = pred_xy - pred_wh_half\n",
    "        pred_maxes   = pred_xy + pred_wh_half    \n",
    "\n",
    "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "\n",
    "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        \n",
    "        true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "        pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "        union_areas = pred_areas + true_areas - intersect_areas\n",
    "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "\n",
    "        best_ious   = tf.reduce_max(iou_scores, axis=4)        \n",
    "        conf_delta *= tf.expand_dims(tf.to_float(best_ious < self.ignore_thresh), 4)\n",
    "\n",
    "        \"\"\"\n",
    "        Compute some online statistics\n",
    "        \"\"\"            \n",
    "        true_xy = true_box_xy / grid_factor\n",
    "        true_wh = tf.exp(true_box_wh) * self.anchors / net_factor\n",
    "\n",
    "        true_wh_half = true_wh / 2.\n",
    "        true_mins    = true_xy - true_wh_half\n",
    "        true_maxes   = true_xy + true_wh_half\n",
    "\n",
    "        pred_xy = pred_box_xy / grid_factor\n",
    "        pred_wh = tf.exp(pred_box_wh) * self.anchors / net_factor \n",
    "        \n",
    "        pred_wh_half = pred_wh / 2.\n",
    "        pred_mins    = pred_xy - pred_wh_half\n",
    "        pred_maxes   = pred_xy + pred_wh_half      \n",
    "\n",
    "        intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
    "        intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
    "        intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
    "        intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
    "        \n",
    "        true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
    "        pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
    "\n",
    "        union_areas = pred_areas + true_areas - intersect_areas\n",
    "        iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
    "        iou_scores  = object_mask * tf.expand_dims(iou_scores, 4)\n",
    "        \n",
    "        count       = tf.reduce_sum(object_mask)\n",
    "        count_noobj = tf.reduce_sum(1 - object_mask)\n",
    "        detect_mask = tf.to_float((pred_box_conf*object_mask) >= 0.5)\n",
    "        class_mask  = tf.expand_dims(tf.to_float(tf.equal(tf.argmax(pred_box_class, -1), true_box_class)), 4)\n",
    "        recall50    = tf.reduce_sum(tf.to_float(iou_scores >= 0.5 ) * detect_mask  * class_mask) / (count + 1e-3)\n",
    "        recall75    = tf.reduce_sum(tf.to_float(iou_scores >= 0.75) * detect_mask  * class_mask) / (count + 1e-3)    \n",
    "        avg_iou     = tf.reduce_sum(iou_scores) / (count + 1e-3)\n",
    "        avg_obj     = tf.reduce_sum(pred_box_conf  * object_mask)  / (count + 1e-3)\n",
    "        avg_noobj   = tf.reduce_sum(pred_box_conf  * (1-object_mask))  / (count_noobj + 1e-3)\n",
    "        avg_cat     = tf.reduce_sum(object_mask * class_mask) / (count + 1e-3) \n",
    "\n",
    "        \"\"\"\n",
    "        Warm-up training\n",
    "        \"\"\"\n",
    "        batch_seen = tf.assign_add(batch_seen, 1.)\n",
    "        \n",
    "        true_box_xy, true_box_wh, xywh_mask = tf.cond(tf.less(batch_seen, self.warmup_batches+1), \n",
    "                              lambda: [true_box_xy + (0.5 + self.cell_grid[:,:grid_h,:grid_w,:,:]) * (1-object_mask), \n",
    "                                       true_box_wh + tf.zeros_like(true_box_wh) * (1-object_mask), \n",
    "                                       tf.ones_like(object_mask)],\n",
    "                              lambda: [true_box_xy, \n",
    "                                       true_box_wh,\n",
    "                                       object_mask])\n",
    "\n",
    "        \"\"\"\n",
    "        Compare each true box to all anchor boxes\n",
    "        \"\"\"      \n",
    "        wh_scale = tf.exp(true_box_wh) * self.anchors / net_factor\n",
    "        wh_scale = tf.expand_dims(2 - wh_scale[..., 0] * wh_scale[..., 1], axis=4) # the smaller the box, the bigger the scale\n",
    "\n",
    "        xy_delta    = xywh_mask   * (pred_box_xy-true_box_xy) * wh_scale * self.xywh_scale\n",
    "        wh_delta    = xywh_mask   * (pred_box_wh-true_box_wh) * wh_scale * self.xywh_scale\n",
    "        conf_delta  = object_mask * (pred_box_conf-true_box_conf) * self.obj_scale + (1-object_mask) * conf_delta * self.noobj_scale\n",
    "        class_delta = object_mask * \\\n",
    "                      tf.expand_dims(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class), 4) * \\\n",
    "                      self.class_scale\n",
    "\n",
    "        loss_xy    = tf.reduce_sum(tf.square(xy_delta),       list(range(1,5)))\n",
    "        loss_wh    = tf.reduce_sum(tf.square(wh_delta),       list(range(1,5)))\n",
    "        loss_conf  = tf.reduce_sum(tf.square(conf_delta),     list(range(1,5)))\n",
    "        loss_class = tf.reduce_sum(class_delta,               list(range(1,5)))\n",
    "\n",
    "        loss = loss_xy + loss_wh + loss_conf + loss_class\n",
    "\n",
    "        loss = tf.Print(loss, [grid_h, avg_obj], message='avg_obj \\t\\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [grid_h, avg_noobj], message='avg_noobj \\t\\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [grid_h, avg_iou], message='avg_iou \\t\\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [grid_h, avg_cat], message='avg_cat \\t\\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [grid_h, recall50], message='recall50 \\t', summarize=1000)\n",
    "        loss = tf.Print(loss, [grid_h, recall75], message='recall75 \\t', summarize=1000)   \n",
    "        loss = tf.Print(loss, [grid_h, count], message='count \\t', summarize=1000)     \n",
    "        loss = tf.Print(loss, [grid_h, tf.reduce_sum(loss_xy), \n",
    "                                       tf.reduce_sum(loss_wh), \n",
    "                                       tf.reduce_sum(loss_conf), \n",
    "                                       tf.reduce_sum(loss_class)],  message='loss xy, wh, conf, class: \\t',   summarize=1000)   \n",
    "\n",
    "\n",
    "        return loss*self.grid_scale\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [(None, 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image = Input(shape=(None, None, 3)) # net_h, net_w, 3\n",
    "true_boxes  = Input(shape=(1, 1, 1, max_box_per_image, 4))\n",
    "#                          None, None,len(anchors)//6\n",
    "true_yolo_1 = Input(shape=(None, None, 3, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\n",
    "true_yolo_2 = Input(shape=(None, None, 3, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\n",
    "true_yolo_3 = Input(shape=(None, None, 3, 4+1+nb_class)) # grid_h, grid_w, nb_anchor, 5+nb_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_grid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-9a228293a00e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[0manchors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m33\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m23\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m61\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m62\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m45\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m59\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m119\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m116\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m156\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m198\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;36m373\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m326\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m loss_yolo_1 = YoloLayer(anchors[12:], \n\u001b[0;32m--> 610\u001b[0;31m                         \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmax_grid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    611\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m                         \u001b[0mwarmup_batches\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_grid' is not defined"
     ]
    }
   ],
   "source": [
    "# Conv\n",
    "# Layer 1\n",
    "\n",
    "lay1_1 = Conv2D(32, 3, strides=(1, 1), padding='same')(input_image)\n",
    "lay1_2 = BatchNormalization(epsilon=0.001)(lay1_1)\n",
    "lay1_3 = LeakyReLU(alpha=0.1)(lay1_2)\n",
    "\n",
    "# Downsample\n",
    "\n",
    "# Conv\n",
    "# Layer 2\n",
    "\n",
    "lay2_1 = ZeroPadding2D(((1,0),(1,0)))(lay1_3)\n",
    "lay2_2 = Conv2D(64, 3, strides=(2, 2), padding='valid')(lay2_1)\n",
    "lay2_3 = BatchNormalization(epsilon=0.001)(lay2_2)\n",
    "lay2_4 = LeakyReLU(alpha=0.1)(lay2_3)\n",
    "\n",
    "# Conv\n",
    "# Layer 3\n",
    "\n",
    "lay3_1 = Conv2D(32, 1, strides=(1, 1), padding='same')(lay2_4)\n",
    "lay3_2 = BatchNormalization(epsilon=0.001)(lay3_1)\n",
    "lay3_3 = LeakyReLU(alpha=0.1)(lay3_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 4\n",
    "\n",
    "lay4_1 = Conv2D(64, 3, strides=(1, 1), padding='same')(lay3_3)\n",
    "lay4_2 = BatchNormalization(epsilon=0.001)(lay4_1)\n",
    "lay4_3 = LeakyReLU(alpha=0.1)(lay4_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 5\n",
    "\n",
    "lay5_0 = concatenate([lay2_4,lay4_3], axis=-1)\n",
    "lay5_1 = Conv2D(32, 1, strides=(1, 1), padding='same')(lay5_0)\n",
    "lay5_2 = linear(lay5_1)\n",
    "\n",
    "# Downsample\n",
    "\n",
    "# Conv\n",
    "# Layer 6\n",
    "\n",
    "lay6_1 = ZeroPadding2D(((1,0),(1,0)))(lay5_2)\n",
    "lay6_2 = Conv2D(128, 3, strides=(2, 2), padding='valid')(lay6_1)\n",
    "lay6_3 = BatchNormalization(epsilon=0.001)(lay6_2)\n",
    "lay6_4 = LeakyReLU(alpha=0.1)(lay6_3)\n",
    "\n",
    "# Conv\n",
    "# Layer 7\n",
    "\n",
    "lay7_1 = Conv2D(64, 1, strides=(1, 1), padding='same')(lay6_4)\n",
    "lay7_2 = BatchNormalization(epsilon=0.001)(lay7_1)\n",
    "lay7_3 = LeakyReLU(alpha=0.1)(lay7_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 8\n",
    "\n",
    "lay8_1 = Conv2D(128, 3, strides=(1, 1), padding='same')(lay7_3)\n",
    "lay8_2 = BatchNormalization(epsilon=0.001)(lay8_1)\n",
    "lay8_3 = LeakyReLU(alpha=0.1)(lay8_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 9\n",
    "lay9_0 = concatenate([lay6_3,lay8_3], axis=-1)\n",
    "lay9_1 = Conv2D(64, 1, strides=(1, 1), padding='same')(lay9_0)\n",
    "lay9_2 = linear(lay9_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 10\n",
    "\n",
    "lay10_1 = Conv2D(64, 1, strides=(1, 1), padding='same')(lay9_2)\n",
    "lay10_2 = BatchNormalization(epsilon=0.001)(lay10_1)\n",
    "lay10_3 = LeakyReLU(alpha=0.1)(lay10_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 11\n",
    "\n",
    "lay11_1 = Conv2D(128, 3, strides=(1, 1), padding='same')(lay10_3)\n",
    "lay11_2 = BatchNormalization(epsilon=0.001)(lay11_1)\n",
    "lay11_3 = LeakyReLU(alpha=0.1)(lay11_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# layer 12\n",
    "\n",
    "lay12_0 = concatenate([lay9_2,lay11_3], axis=-1)\n",
    "lay12_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay12_0)\n",
    "lay12_2 = linear(lay9_1)\n",
    "\n",
    "# Downsample\n",
    "\n",
    "# Conv \n",
    "# Layer 13\n",
    "\n",
    "lay13_1 = ZeroPadding2D(((1,0),(1,0)))(lay12_2)\n",
    "lay13_2 = Conv2D(256, 3, strides=(2, 2), padding='valid')(lay13_1)\n",
    "lay13_3 = BatchNormalization(epsilon=0.001)(lay13_2)\n",
    "lay13_4 = LeakyReLU(alpha=0.1)(lay13_3)\n",
    "\n",
    "# Conv\n",
    "# Layer 14\n",
    "\n",
    "lay14_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay13_4)\n",
    "lay14_2 = BatchNormalization(epsilon=0.001)(lay14_1)\n",
    "lay14_3 = LeakyReLU(alpha=0.1)(lay14_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 15\n",
    "\n",
    "lay15_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay14_3)\n",
    "lay15_2 = BatchNormalization(epsilon=0.001)(lay15_1)\n",
    "lay15_3 = LeakyReLU(alpha=0.1)(lay15_2)\n",
    "\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 16\n",
    "\n",
    "lay16_0 = concatenate([lay13_4,lay15_3], axis=-1)\n",
    "lay16_1 = Conv2D(64, 1, strides=(1, 1), padding='same')(lay16_0)\n",
    "lay16_2 = linear(lay16_1)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 17\n",
    "\n",
    "lay17_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay16_2)\n",
    "lay17_2 = BatchNormalization(epsilon=0.001)(lay17_1)\n",
    "lay17_3 = LeakyReLU(alpha=0.1)(lay17_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 18\n",
    "\n",
    "lay18_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay17_3)\n",
    "lay18_2 = BatchNormalization(epsilon=0.001)(lay18_1)\n",
    "lay18_3 = LeakyReLU(alpha=0.1)(lay18_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 19\n",
    "\n",
    "lay19_0 = concatenate([lay16_2,lay18_3], axis=-1)\n",
    "lay19_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay19_0)\n",
    "lay19_2 = linear(lay19_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 20\n",
    "\n",
    "lay20_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay19_2)\n",
    "lay20_2 = BatchNormalization(epsilon=0.001)(lay20_1)\n",
    "lay20_3 = LeakyReLU(alpha=0.1)(lay20_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 21\n",
    "\n",
    "lay21_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay20_3)\n",
    "lay21_2 = BatchNormalization(epsilon=0.001)(lay21_1)\n",
    "lay21_3 = LeakyReLU(alpha=0.1)(lay21_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 22\n",
    "\n",
    "lay22_0 = concatenate([lay19_2,lay21_3], axis=-1)\n",
    "lay22_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay22_0)\n",
    "lay22_2 = linear(lay22_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 23\n",
    "\n",
    "lay23_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay22_2)\n",
    "lay23_2 = BatchNormalization(epsilon=0.001)(lay23_1)\n",
    "lay23_3 = LeakyReLU(alpha=0.1)(lay23_2)\n",
    "\n",
    "# Conv \n",
    "# Layer 24 \n",
    "\n",
    "lay24_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay23_3)\n",
    "lay24_2 = BatchNormalization(epsilon=0.001)(lay24_1)\n",
    "lay24_3 = LeakyReLU(alpha=0.1)(lay24_2)\n",
    "\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 25\n",
    "\n",
    "lay25_0 = concatenate([lay22_2,lay24_3], axis=-1)\n",
    "lay25_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay25_0)\n",
    "lay25_2 = linear(lay25_1)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 26\n",
    "\n",
    "lay26_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay25_2)\n",
    "lay26_2 = BatchNormalization(epsilon=0.001)(lay26_1)\n",
    "lay26_3 = LeakyReLU(alpha=0.1)(lay26_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 27\n",
    "\n",
    "lay27_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay26_2)\n",
    "lay27_2 = BatchNormalization(epsilon=0.001)(lay27_1)\n",
    "lay27_3 = LeakyReLU(alpha=0.1)(lay27_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 28\n",
    "\n",
    "lay28_0 = concatenate([lay27_3,lay25_2], axis=-1)\n",
    "lay28_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay28_0)\n",
    "lay28_2 = linear(lay28_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 29\n",
    "\n",
    "lay29_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay28_2)\n",
    "lay29_2 = BatchNormalization(epsilon=0.001)(lay29_1)\n",
    "lay29_3 = LeakyReLU(alpha=0.1)(lay29_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 30\n",
    "\n",
    "lay30_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay29_3)\n",
    "lay30_2 = BatchNormalization(epsilon=0.001)(lay30_1)\n",
    "lay30_3 = LeakyReLU(alpha=0.1)(lay30_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 31\n",
    "\n",
    "lay31_0 = concatenate([lay30_3,lay28_2], axis=-1)\n",
    "lay31_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay31_0)\n",
    "lay31_2 = linear(lay31_1)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 32\n",
    "\n",
    "lay32_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay31_2)\n",
    "lay32_2 = BatchNormalization(epsilon=0.001)(lay32_1)\n",
    "lay32_3 = LeakyReLU(alpha=0.1)(lay32_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 33\n",
    "\n",
    "lay33_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay32_3)\n",
    "lay33_2 = BatchNormalization(epsilon=0.001)(lay33_1)\n",
    "lay33_3 = LeakyReLU(alpha=0.1)(lay33_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 34\n",
    "\n",
    "lay34_0 = concatenate([lay33_3,lay31_2], axis=-1)\n",
    "lay34_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay34_0)\n",
    "lay34_2 = linear(lay34_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 35\n",
    "\n",
    "lay35_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay34_2)\n",
    "lay35_2 = BatchNormalization(epsilon=0.001)(lay33_1)\n",
    "lay35_3 = LeakyReLU(alpha=0.1)(lay33_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 36\n",
    "\n",
    "lay36_1 = Conv2D(256, 3, strides=(1, 1), padding='same')(lay35_3)\n",
    "lay36_2 = BatchNormalization(epsilon=0.001)(lay36_1)\n",
    "lay36_3 = LeakyReLU(alpha=0.1)(lay36_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 37\n",
    "\n",
    "lay37_0 = concatenate([lay36_3,lay34_2], axis=-1)\n",
    "lay37_1 = Conv2D(128, 1, strides=(1, 1), padding='same')(lay37_0)\n",
    "lay37_2 = linear(lay37_1)\n",
    "\n",
    "# Downsample\n",
    "\n",
    "# Conv\n",
    "# Layer 38\n",
    "\n",
    "lay38_1 = ZeroPadding2D(((1,0),(1,0)))(lay37_2)\n",
    "lay38_2 = Conv2D(512, 3, strides=(2, 2), padding='valid')(lay38_1)\n",
    "lay38_3 = BatchNormalization(epsilon=0.001)(lay38_2)\n",
    "lay38_4 = LeakyReLU(alpha=0.1)(lay38_3)\n",
    "\n",
    "# Conv\n",
    "# Layer 39\n",
    "\n",
    "lay39_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay38_4)\n",
    "lay39_2 = BatchNormalization(epsilon=0.001)(lay39_1)\n",
    "lay39_3 = LeakyReLU(alpha=0.1)(lay39_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 40\n",
    "\n",
    "lay40_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay39_3)\n",
    "lay40_2 = BatchNormalization(epsilon=0.001)(lay40_1)\n",
    "lay40_3 = LeakyReLU(alpha=0.1)(lay40_2)\n",
    "\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 41\n",
    "\n",
    "lay41_0 = concatenate([lay40_3,lay38_4], axis=-1)\n",
    "lay41_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay41_0)\n",
    "lay41_2 = linear(lay41_1)\n",
    "\n",
    "# Conv\n",
    "# Layer 42\n",
    "\n",
    "lay42_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay41_2)\n",
    "lay42_2 = BatchNormalization(epsilon=0.001)(lay42_1)\n",
    "lay42_3 = LeakyReLU(alpha=0.1)(lay42_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 43\n",
    "\n",
    "lay43_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay42_2)\n",
    "lay43_2 = BatchNormalization(epsilon=0.001)(lay43_1)\n",
    "lay43_3 = LeakyReLU(alpha=0.1)(lay43_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 44\n",
    "\n",
    "lay44_1 = concatenate([lay43_3,lay41_2], axis=-1)\n",
    "lay44_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay44_1)\n",
    "lay44_3 = linear(lay44_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 45\n",
    "\n",
    "lay45_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay44_3)\n",
    "lay45_2 = BatchNormalization(epsilon=0.001)(lay45_1)\n",
    "lay45_3 = LeakyReLU(alpha=0.1)(lay45_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 46\n",
    "\n",
    "lay46_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay45_3)\n",
    "lay46_2 = BatchNormalization(epsilon=0.001)(lay46_1)\n",
    "lay46_3 = LeakyReLU(alpha=0.1)(lay46_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 47\n",
    "\n",
    "lay47_1 = concatenate([lay46_3,lay44_3], axis=-1)\n",
    "lay47_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay47_1)\n",
    "lay47_3 = linear(lay47_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 48\n",
    "\n",
    "lay48_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay47_3)\n",
    "lay48_2 = BatchNormalization(epsilon=0.001)(lay48_1)\n",
    "lay48_3 = LeakyReLU(alpha=0.1)(lay48_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 49\n",
    "\n",
    "lay49_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay48_3)\n",
    "lay49_2 = BatchNormalization(epsilon=0.001)(lay49_1)\n",
    "lay49_3 = LeakyReLU(alpha=0.1)(lay49_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 50\n",
    "\n",
    "lay50_1 = concatenate([lay49_3,lay47_3], axis=-1)\n",
    "lay50_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay50_1)\n",
    "lay50_3 = linear(lay50_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 51\n",
    "\n",
    "lay51_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay50_3)\n",
    "lay51_2 = BatchNormalization(epsilon=0.001)(lay51_1)\n",
    "lay51_3 = LeakyReLU(alpha=0.1)(lay51_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 52\n",
    "\n",
    "lay52_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay51_3)\n",
    "lay52_2 = BatchNormalization(epsilon=0.001)(lay52_1)\n",
    "lay52_3 = LeakyReLU(alpha=0.1)(lay52_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 53\n",
    "\n",
    "lay53_1 = concatenate([lay52_3,lay50_2], axis=-1)\n",
    "lay53_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay53_1)\n",
    "lay53_3 = linear(lay53_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 54\n",
    "\n",
    "lay54_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay53_3)\n",
    "lay54_2 = BatchNormalization(epsilon=0.001)(lay54_1)\n",
    "lay54_3 = LeakyReLU(alpha=0.1)(lay54_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 55\n",
    "\n",
    "lay55_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay54_3)\n",
    "lay55_2 = BatchNormalization(epsilon=0.001)(lay55_1)\n",
    "lay55_3 = LeakyReLU(alpha=0.1)(lay55_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 56\n",
    "\n",
    "lay56_1 = concatenate([lay55_3,lay53_3], axis=-1)\n",
    "lay56_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay56_1)\n",
    "lay56_3 = linear(lay56_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 57\n",
    "\n",
    "lay57_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay56_3)\n",
    "lay57_2 = BatchNormalization(epsilon=0.001)(lay57_1)\n",
    "lay57_3 = LeakyReLU(alpha=0.1)(lay57_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 58\n",
    "\n",
    "lay58_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay57_3)\n",
    "lay58_2 = BatchNormalization(epsilon=0.001)(lay58_1)\n",
    "lay58_3 = LeakyReLU(alpha=0.1)(lay58_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 59\n",
    "\n",
    "lay59_1 = concatenate([lay58_3,lay56_3], axis=-1)\n",
    "lay59_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay59_1)\n",
    "lay59_3 = linear(lay59_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 60\n",
    "\n",
    "lay60_1 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay59_3)\n",
    "lay60_2 = BatchNormalization(epsilon=0.001)(lay60_1)\n",
    "lay60_3 = LeakyReLU(alpha=0.1)(lay60_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 61\n",
    "\n",
    "lay61_1 = Conv2D(512, 3, strides=(1, 1), padding='same')(lay60_3)\n",
    "lay61_2 = BatchNormalization(epsilon=0.001)(lay61_1)\n",
    "lay61_3 = LeakyReLU(alpha=0.1)(lay61_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 62\n",
    "\n",
    "lay62_1 = concatenate([lay61_3,lay59_3], axis=-1)\n",
    "lay62_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay62_1)\n",
    "lay62_3 = linear(lay62_2)\n",
    "\n",
    "# # Downsample\n",
    "\n",
    "# Conv\n",
    "# Layer 63\n",
    "\n",
    "lay63_1 = ZeroPadding2D(((1,0),(1,0)))(lay62_3)\n",
    "lay63_2 = Conv2D(1024, 3, strides=(2, 2), padding='valid')(lay63_1)\n",
    "lay63_3 = BatchNormalization(epsilon=0.001)(lay63_2)\n",
    "lay63_4 = LeakyReLU(alpha=0.1)(lay63_3)\n",
    "\n",
    "# Conv\n",
    "# Layer 64\n",
    "\n",
    "lay64_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay63_4)\n",
    "lay64_2 = BatchNormalization(epsilon=0.001)(lay64_1)\n",
    "lay64_3 = LeakyReLU(alpha=0.1)(lay64_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 65\n",
    "\n",
    "lay65_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay64_3)\n",
    "lay65_2 = BatchNormalization(epsilon=0.001)(lay65_1)\n",
    "lay65_3 = LeakyReLU(alpha=0.1)(lay65_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 66\n",
    "\n",
    "lay66_1 = concatenate([lay65_3,lay63_3], axis=-1)\n",
    "lay66_2 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay66_1)\n",
    "lay66_3 = linear(lay66_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 67\n",
    "\n",
    "lay67_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay66_3)\n",
    "lay67_2 = BatchNormalization(epsilon=0.001)(lay67_1)\n",
    "lay67_3 = LeakyReLU(alpha=0.1)(lay67_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 68\n",
    "\n",
    "lay68_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay67_3)\n",
    "lay68_2 = BatchNormalization(epsilon=0.001)(lay65_1)\n",
    "lay68_3 = LeakyReLU(alpha=0.1)(lay65_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 69\n",
    "\n",
    "lay69_1 = concatenate([lay68_3,lay66_3], axis=-1)\n",
    "lay69_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay69_1)\n",
    "lay69_3 = linear(lay69_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 70\n",
    "\n",
    "lay70_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay69_3)\n",
    "lay70_2 = BatchNormalization(epsilon=0.001)(lay70_1)\n",
    "lay70_3 = LeakyReLU(alpha=0.1)(lay70_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 71\n",
    "\n",
    "lay71_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay70_3)\n",
    "lay71_2 = BatchNormalization(epsilon=0.001)(lay71_1)\n",
    "lay71_3 = LeakyReLU(alpha=0.1)(lay71_2)\n",
    "\n",
    "# # Shortcut -> Conv\n",
    "# Layer 72\n",
    "\n",
    "lay72_1 = concatenate([lay71_3,lay69_3], axis=-1)\n",
    "lay72_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay72_1)\n",
    "lay72_3 = linear(lay72_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 73\n",
    "\n",
    "lay73_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay72_3)\n",
    "lay73_2 = BatchNormalization(epsilon=0.001)(lay73_1)\n",
    "lay73_3 = LeakyReLU(alpha=0.1)(lay73_2)\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 74\n",
    "\n",
    "lay74_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay73_3)\n",
    "lay74_2 = BatchNormalization(epsilon=0.001)(lay74_1)\n",
    "lay74_3 = LeakyReLU(alpha=0.1)(lay74_2)\n",
    "\n",
    "# Shortcut -> Conv\n",
    "# Layer 75\n",
    "\n",
    "lay75_1 = concatenate([lay74_3,lay72_3], axis=-1)\n",
    "lay75_2 = Conv2D(256, 1, strides=(1, 1), padding='same')(lay75_1)\n",
    "lay75_3 = linear(lay75_2)\n",
    "\n",
    "# ######################\n",
    "\n",
    "# Conv\n",
    "# Layer 76\n",
    "\n",
    "lay76_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay75_3)\n",
    "lay76_2 = BatchNormalization(epsilon=0.001)(lay76_1)\n",
    "lay76_3 = LeakyReLU(alpha=0.1)(lay76_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 77\n",
    "\n",
    "lay77_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay76_3)\n",
    "lay77_2 = BatchNormalization(epsilon=0.001)(lay77_1)\n",
    "lay77_3 = LeakyReLU(alpha=0.1)(lay77_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 78\n",
    "\n",
    "lay78_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay77_3)\n",
    "lay78_2 = BatchNormalization(epsilon=0.001)(lay78_1)\n",
    "lay78_3 = LeakyReLU(alpha=0.1)(lay78_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 79\n",
    "\n",
    "lay79_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay78_3)\n",
    "lay79_2 = BatchNormalization(epsilon=0.001)(lay79_1)\n",
    "lay79_3 = LeakyReLU(alpha=0.1)(lay79_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 80\n",
    "\n",
    "lay80_1 = Conv2D(512, 1, strides=(1, 1), padding='same')(lay79_3)\n",
    "lay80_2 = BatchNormalization(epsilon=0.001)(lay80_1)\n",
    "lay80_3 = LeakyReLU(alpha=0.1)(lay80_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 81\n",
    "\n",
    "lay81_1 = Conv2D(1024, 3, strides=(1, 1), padding='same')(lay80_3)\n",
    "lay81_2 = BatchNormalization(epsilon=0.001)(lay81_1)\n",
    "lay81_3 = LeakyReLU(alpha=0.1)(lay81_2)\n",
    "\n",
    "# Conv\n",
    "# Layer 82\n",
    "\n",
    "lay82_1 = Conv2D(255, 1, strides=(1, 1), padding='same')(lay81_3)\n",
    "lay82_2 = BatchNormalization(epsilon=0.001)(lay82_1)\n",
    "lay82_3 = LeakyReLU(alpha=0.1)(lay82_2)\n",
    "\n",
    "pred_yolo_1 = Conv2D((3*(5+nb_class)), 1, strides=(1, 1), padding='same')(lay82_3)\n",
    "anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "ignore_thresh = .7\n",
    "\n",
    "loss_yolo_1 = YoloLayer(anchors[12:], \n",
    "                        [1*num for num in max_grid], \n",
    "                        batch_size, \n",
    "                        warmup_batches, \n",
    "                        ignore_thresh, \n",
    "                        grid_scales[0],\n",
    "                        obj_scale,\n",
    "                        noobj_scale,\n",
    "                        xywh_scale,\n",
    "                        class_scale)([input_image, pred_yolo_1, true_yolo_1, true_boxes])\n",
    "# [yolo] 83\n",
    "# mask = 6,7,8\n",
    "\n",
    "# classes=80\n",
    "# num=9\n",
    "# jitter=.3\n",
    "\n",
    "# truth_thresh = 1\n",
    "# random=1\n",
    "\n",
    "\n",
    "# [route] 84\n",
    "# layers = -4\n",
    "\n",
    "# Conv\n",
    "# Layer 85\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=256\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# [upsample] 86\n",
    "# stride=2\n",
    "\n",
    "# [route] 87\n",
    "# layers = -1, 61\n",
    "\n",
    "\n",
    "\n",
    "# Conv\n",
    "# Layer 88\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=256\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 89\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=512\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 90\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=256\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 91\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=512\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 92\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=256\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 93\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=512\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 94\n",
    "\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=255\n",
    "# activation=linear\n",
    "\n",
    "\n",
    "# [yolo] 95\n",
    "# mask = 3,4,5\n",
    "# anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "# classes=80\n",
    "# num=9\n",
    "# jitter=.3\n",
    "# ignore_thresh = .7\n",
    "# truth_thresh = 1\n",
    "# random=1\n",
    "\n",
    "\n",
    "\n",
    "# [route] 96\n",
    "# layers = -4\n",
    "\n",
    "# Conv\n",
    "# Layer 97\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=128\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# [upsample] 98\n",
    "# stride=2\n",
    "\n",
    "# [route] 99\n",
    "# layers = -1, 36\n",
    "\n",
    "\n",
    "\n",
    "# Conv 100\n",
    "# batch_normalize=1\n",
    "# filters=128\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 101\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=256\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 102\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=128\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 103\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=256\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 104\n",
    "\n",
    "# batch_normalize=1\n",
    "# filters=128\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 105\n",
    "\n",
    "# batch_normalize=1\n",
    "# size=3\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=256\n",
    "# activation=leaky\n",
    "\n",
    "# Conv\n",
    "# Layer 106\n",
    "\n",
    "# size=1\n",
    "# stride=1\n",
    "# pad=1\n",
    "# filters=255\n",
    "# activation=linear\n",
    "\n",
    "\n",
    "# [yolo] 107\n",
    "# mask = 0,1,2\n",
    "# anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326\n",
    "# classes=80\n",
    "# num=9\n",
    "# jitter=.3\n",
    "# ignore_thresh = .7\n",
    "# truth_thresh = 1\n",
    "# random=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=input_image, outputs=lay82_3)\n",
    "plot_model(model, to_file='model.png',show_shapes='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
